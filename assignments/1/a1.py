

#
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os

#                       
#UNDERSTAND THIS PART  :GENERATED BY LMM 
# TASK 2
# 2.2.1  
def task2_2_1():
    # Load the CSV file with only the relevant columns
    columns = [
        'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
        'time_signature', 'track_genre'
    ]

    df = pd.read_csv(r'./data/external/spotify.csv', usecols=columns)

    # One-hot encode the 'track_genre' column
    df_encoded = pd.get_dummies(df, columns=['track_genre'])

    # Select only the feature columns and the encoded genre columns
    features = [
        'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
        'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
        'time_signature'
    ]

    # Calculate the Spearman correlation matrix
    corr_matrix = df_encoded[features + list(df_encoded.columns[df_encoded.columns.str.startswith('track_genre')])].corr(method='spearman')

    # Extract the correlations between the features and the track genres
    genre_corr = corr_matrix.loc[features, df_encoded.columns[df_encoded.columns.str.startswith('track_genre')]]

    # Compute the average absolute correlation for each feature across all genres
    avg_corr = genre_corr.abs().mean(axis=1)

    # Sort features by their average correlation
    relevant_features = avg_corr.sort_values(ascending=False)

    # Set the directory and filename for saving the figure
    directory = 'assignments/1/figures'
    filename = 'figure1.png'

    # Create the directory if it doesn't exist
    os.makedirs(directory, exist_ok=True)

    # Visualize the most relevant features
    plt.figure(figsize=(10, 6))
    sns.barplot(x=relevant_features.values, y=relevant_features.index, palette='viridis')
    plt.title('Most Relevant Features for Genre Classification (Average Spearman Correlation)')
    plt.xlabel('Average Absolute Correlation')

    # Save the figure
    plt.savefig(os.path.join(directory, filename))


    print(f"Figure saved as {os.path.join(directory, filename)}")

    # 1. Visualize the distribution of each feature
    plt.figure(figsize=(15, 12))
    for i, feature in enumerate(features, 1):
        plt.subplot(4, 3, i)
        sns.histplot(df[feature], kde=True)
        plt.title(f'Distribution of {feature}')
    plt.tight_layout()
    plt.savefig(os.path.join(directory, "figure2"))
    print("done")


# task2_2_1()



#########################################################

#2_3

#2_3_1 done in models/knn/knn.py and performance_measures/performance.py took care


#########################
from models.knn.knn import * #-m flag allows you to run a module as if it were a script while maintaining its context within the package hierarchy.
import pandas as pd
import numpy as np
import argparse

###########33
# data
columns = [
    'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
    'time_signature', 'track_genre'
]

# Load the CSV file with only the relevant columns
df = pd.read_csv(r'./data/external/spotify.csv', usecols=columns)

# Shuffle the rows of the DataFrame
df = df.sample(frac=1).reset_index(drop=True)
print(df.shape)



# Initialize the argument parser
parser = argparse.ArgumentParser(description="Process an integer argument.")
parser.add_argument('--data_sample', type=int, required=True, help='An integer number to be processed')
args = parser.parse_args()
print(f"The integer passed is: {args.data_sample}")
# df=df[:(args.data_sample)]

# Convert dataframe to numpy array
data = df.values
np.random.seed(42)  # For reproducibility
np.random.shuffle(data)  # Shuffle the data

# Calculate split indices
total_size = len(data)
train_size = int(0.8 * total_size)
temp_size = total_size - train_size
val_size = test_size = int(0.1 * total_size)

# Split the data
train_data = data[:train_size]
temp_data = data[train_size:]
test_data = temp_data[:test_size]
val_data = temp_data[test_size:]

# Convert numpy arrays back to pandas dataframes
train_df = pd.DataFrame(train_data, columns=columns)
test_df = pd.DataFrame(test_data, columns=columns)
val_df = pd.DataFrame(val_data, columns=columns)

float_columns = [
    'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',
    'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',
    'time_signature'
]

# Calculate min and max values from the training set
train_min = train_df[float_columns].min()
train_max = train_df[float_columns].max()

# Normalize the float columns in the training set
train_df[float_columns] = (train_df[float_columns] - train_min) / (train_max - train_min+1e-6)

# Normalize the float columns in the test set using the same min-max values as the training set
test_df[float_columns] = (test_df[float_columns] - train_min) / (train_max - train_min+1e-6)

# Normalize the float columns in the validation set using the same min-max values as the training set
val_df[float_columns] = (val_df[float_columns] - train_min) / (train_max - train_min+1e-6)


# Save the splits into separate CSV files
train_df.to_csv('./data/interim/spotify_train.csv', index=False)
test_df.to_csv('./data/interim/spotify_test.csv', index=False)
val_df.to_csv('./data/interim/spotify_val.csv', index=False)


exit()
#######################################33 saving in interim after splitting






import concurrent.futures
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def task3_1_3(): #task 3 :1. 2. 3. for 3 made heatmap
    #######################
    # Load the split datasets
    train_df = pd.read_csv('./data/interim/spotify_train.csv')
    test_df = pd.read_csv('./data/interim/spotify_test.csv')
    val_df = pd.read_csv('./data/interim/spotify_val.csv')

    # Extract features and target variables
    X_train = train_df.drop('track_genre', axis=1).values
    print(X_train.shape)
    Y_train = train_df['track_genre'].values
    X_test = test_df.drop('track_genre', axis=1).values
    Y_test = test_df['track_genre'].values
    X_val = val_df.drop('track_genre', axis=1).values
    Y_val = val_df['track_genre'].values

    # Initialize and use KNearestNeighbours
    #Experiment with k
    # knn = KNearestNeighbours(k=3, distance_metric='euclidean',prediction_type='most_common')
    # Fit the model
    # knn.fit(X_train, Y_train)
    # knn.validate(X_val, Y_val)
    # knn.test(X_test, Y_test)

    import concurrent.futures
    dis_metric=['euclidean','manhattan','cosine','hamming'] #['euclidean','manhattan','cosine''hamming']
    pred_type= [ 'most_common','weighted_sum'] #['most_common','weighted_sum']

    ######################3


    # Initialize variables
    best_accuracy = 0
    best_params = {'k': None, 'distance_metric': None, 'prediction_type': None}

    # Store results for plotting
    results = []

    def evaluate_knn(k, pred):
        local_best_accuracy = 0
        local_best_params = None
        local_results = []

        for dis in dis_metric:
            # Initialize and fit the KNN model
            knn = KNearestNeighbours(k=k, distance_metric=dis, prediction_type=pred)
            knn.fit(X_train, Y_train)
            # Validate the model
            metrics = knn.validate(X_val, Y_val)
            accuracy = metrics['accuracy']
            # Print the results
            print(f"k = {k}, dis_metric = {dis}, pred_type = {pred}, accuracy = {accuracy}")
            # Track the best accuracy and parameters
            if accuracy > local_best_accuracy:
                local_best_accuracy = accuracy
                local_best_params = {'k': k, 'distance_metric': dis, 'prediction_type': pred}
            # Store the results for plotting
            local_results.append({'k': k, 'distance_metric': dis, 'prediction_type': pred, 'accuracy': accuracy})
        
        return local_best_accuracy, local_best_params, local_results

    # Use ThreadPoolExecutor to parallelize the loop over k
    for pred in pred_type:
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {executor.submit(evaluate_knn, k, pred): k for k in range(1, 20, 2)}
            for future in concurrent.futures.as_completed(futures):
                local_best_accuracy, local_best_params, local_results = future.result()
                results.extend(local_results)
                # Update the global best accuracy and parameters if necessary
                if local_best_accuracy > best_accuracy:
                    best_accuracy = local_best_accuracy
                    best_params = local_best_params

    # Convert results to a DataFrame for heatmap plotting
    df_results = pd.DataFrame(results)


    # Initialize variables to track the best parameters
    best_params = {}
    best_accuracy = -1

    # List to store all results for ranking
    all_results = []

    # Create heatmaps for each prediction type
    for pred in pred_type:
        pred_results = df_results[df_results['prediction_type'] == pred]
        
        # Use pivot_table to handle possible duplicate entries
        pivot_table = pred_results.pivot_table(index="distance_metric", columns="k", values="accuracy", aggfunc=np.mean)
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(pivot_table, annot=True, fmt=".4f", cmap="YlGnBu", cbar=True, 
                    xticklabels=sorted(df_results['k'].unique()), yticklabels=dis_metric)
        plt.title(f'Accuracy Heatmap vs k and Distance Metric ({pred} Prediction Type)')
        plt.xlabel('k')
        plt.ylabel('Distance Metric')
        plt.savefig(f"./assignments/1/figures/{pred}.png")
        plt.close()

        # Add results for ranking
        all_results.append(pred_results)

    # Concatenate all results into a single DataFrame for ranking
    df_all_results = pd.concat(all_results)

    # Find the best combination across all prediction types
    best_combination = df_all_results.loc[df_all_results['accuracy'].idxmax()]
    best_params = {
        'k': best_combination['k'],
        'distance_metric': best_combination['distance_metric'],
        'prediction_type': best_combination['prediction_type']
    }
    best_accuracy = best_combination['accuracy']

    # Print the best parameters and accuracy
    print(f"Best parameters: k = {best_params['k']}, distance_metric = {best_params['distance_metric']}, prediction_type = {best_params['prediction_type']}")
    print(f"Best accuracy: {best_accuracy}")

    # Create a rank list of top 10 pairs
    top_10_pairs = df_all_results[['k', 'distance_metric', 'accuracy']].sort_values(by='accuracy', ascending=False).head(10)

    print("Top 10 pairs:")
    print(top_10_pairs)

#TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK 
# UNCOMMENT BELOW TO TEST
# task3_1_3() #why plot i made best heatmap ,extra weighted sum (better in results than most_common) and hamming
# OutPut:
# Top 10 Accuracy Pairs:
# k: 15, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985706
# k: 17, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985702
# k: 19, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985676
# k: 13, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985645
# k: 11, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985639
# k: 9, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985594
# k: 7, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985565
# k: 5, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985532
# k: 19, Distance Metric: Euclidean, Prediction Type: Weighted Sum, Accuracy: 0.985491
# k: 1, Distance Metric: Manhattan, Prediction Type: Weighted Sum, Accuracy: 0.985489






import pandas as pd
import concurrent.futures #just to use cpu's all threads ,giving each knn to each thread
 # Adjust this import to match your KNN implementation

def task3_4_5():
    # Load the split datasets
    train_df = pd.read_csv('./data/interim/spotify_train.csv')
    test_df = pd.read_csv('./data/interim/spotify_test.csv')
    val_df = pd.read_csv('./data/interim/spotify_val.csv')

    # Extract target variables
    Y_train = train_df['track_genre'].values
    Y_val = val_df['track_genre'].values

    # Initialize hyperparameters
    dis_metric = ['manhattan']
    pred_type = ['weighted_sum']
    
    # Initialize variables
    best_accuracy = 0
    best_params = {'k': None, 'distance_metric': None, 'prediction_type': None, 'features': None}
    results = []

    def evaluate_knn(pred, feature_set):
        local_best_accuracy = 0
        local_best_params = None
        local_results = []
        
        X_train_subset = train_df[list(feature_set)].values
        X_val_subset = val_df[list(feature_set)].values

        for dis in dis_metric:
            knn = KNearestNeighbours(k=15, distance_metric=dis, prediction_type=pred)
            knn.fit(X_train_subset, Y_train)
            metrics = knn.validate(X_val_subset, Y_val)
            accuracy = metrics['accuracy']
            print(f"features = {feature_set}, distance_metric = {dis}, prediction_type = {pred}, accuracy = {accuracy}")
            if accuracy > local_best_accuracy:
                local_best_accuracy = accuracy
                local_best_params = {'k': 15, 'distance_metric': dis, 'prediction_type': pred}
            local_results.append({'k': 15, 'distance_metric': dis, 'prediction_type': pred, 'features': feature_set, 'accuracy': accuracy})
        
        return local_best_accuracy, local_best_params, local_results

    # Feature combinations
    feature_combinations = [
        ('danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('danceability', 'energy', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('danceability', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('danceability', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('energy', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature'),
        ('danceability', 'energy', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo'),
        ('danceability', 'energy', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'valence'),
        ('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo'),
        ('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence'),
        ('danceability', 'energy', 'loudness', 'acousticness', 'instrumentalness')
    ]
    
    # Parallelize over feature combinations and prediction types
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = []
        for feature_set in feature_combinations:
            for pred in pred_type:
                futures.append(executor.submit(evaluate_knn, pred, feature_set))
        
        for future in concurrent.futures.as_completed(futures):
            local_best_accuracy, local_best_params, local_results = future.result()
            results.extend(local_results)

    # Sort results by accuracy in descending order
    sorted_results = sorted(results, key=lambda x: x['accuracy'], reverse=True)
    # Get the top 3 results
    top_3_results = sorted_results[:5]

    print("Top 3 results:")
    for result in top_3_results:
        print(f" features {result['features']} , accuracy {result['accuracy']}")


#TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK 
# WITH :k: 15, Distance Metric: Manhattan, Prediction Type: Weighted Sum
# UNCOMMENT BELOW TO TEST
# task3_4_5()
#OUTPUT :
# Top 5 results:
#  features ('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo') , accuracy 0.9858387196060326
#  features ('danceability', 'energy', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature') , accuracy 0.9858156355801785       
#  features ('danceability', 'energy', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo') , accuracy 0.9857602339181286
#  features ('danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature') , accuracy 0.98567405355494  
#  features ('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence') , accuracy 0.9855263157894737







# WITH :k: 15, Distance Metric: Manhattan, Prediction Type: Weighted Sum
# 2.5 Optimization
import pandas as pd
import concurrent.futures #just to use cpu's all threads ,giving each knn to each thread
 # Adjust this import to match your KNN implementation
from sklearn.neighbors import KNeighborsClassifier
import time
import concurrent.futures
from sklearn.neighbors import KNeighborsClassifier
# https://chatgpt.com/share/128857b7-1798-42dc-8a87-c99a6a7aed25
def task_2_5_2():
    # Load the split datasets
    train_df = pd.read_csv('./data/interim/spotify_train.csv')
    test_df = pd.read_csv('./data/interim/spotify_test.csv')
    val_df = pd.read_csv('./data/interim/spotify_val.csv')

    # Extract target variables
    Y_train = train_df['track_genre'].values
    Y_val = val_df['track_genre'].values

    # Initialize hyperparameters
    pred_type = ['weighted_sum']#weighted_sum
    
    # Feature combinations
    feature_combinations = [
        ('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo')
    ]

    def evaluate_knn(pred, feature_set, optimized=0):
        X_train_subset = train_df[list(feature_set)].values
        X_val_subset = val_df[list(feature_set)].values

        print("optimized ",optimized)
        if optimized == 1:
            knn = KNearestNeighbours(k=15, distance_metric='manhattan', prediction_type=pred)
        elif optimized == 0:
            knn = Old_KNearestNeighbours(k=15, distance_metric='manhattan', prediction_type=pred)
        elif optimized == 2:
            knn = KNeighborsClassifier(n_neighbors=15, metric='manhattan',weights='distance') #do weighted sum, remove it ->by default most common
        
        start_time = time.time()
        
        if optimized in [0, 1]:
            knn.fit(X_train_subset, Y_train)
            # metrics = knn.validate(X_val_subset, Y_val)
            # accuracy = metrics['accuracy']
            predictions = knn.predict(X_val_subset) 
            accuracy = np.mean(predictions == Y_val)
        else:
            knn.fit(X_train_subset, Y_train)
            # accuracy = knn.score(X_val_subset, Y_val)
            predictions = knn.predict(X_val_subset)
            accuracy = np.mean(predictions == Y_val)
        
        elapsed_time = time.time() - start_time
        return accuracy, elapsed_time

    def timed_evaluation(optimized):
        return evaluate_knn(pred_type[0], feature_combinations[0], optimized)
    
    results = {}
    # Evaluate different KNN implementations in parallel
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(timed_evaluation, optimized): optimized
            for optimized in [0, 1, 2]
        }
        
        for future in concurrent.futures.as_completed(futures):
            optimized = futures[future]
            try:
                accuracy, elapsed_time = future.result()
                if optimized == 0:
                    label = "Old KNN"
                elif optimized == 1:
                    label = "New KNN"
                elif optimized == 2:
                    label = "Sklearn KNN"
                
                results[label] = elapsed_time
                print(f"{label} accuracy {accuracy}, time {elapsed_time}")
            except Exception as e:
                print(f"Error occurred: {e}")
    # Plotting the inference times with two y-axis scales
    
    labels = list(results.keys())
    times = list(results.values())

    fig, ax1 = plt.subplots(figsize=(10, 6))

    color = 'tab:blue'
    ax1.set_xlabel('KNN Model')
    ax1.set_ylabel('Inference Time (seconds)', color=color)

    # Create a bar plot and a line plot on the same axis
    ax1.bar(labels, times, color=color, alpha=0.6, label='Bar plot')
    ax1.plot(labels, times, color='tab:red', marker='o', label='Line plot')

    # Set y-ticks at 2-second intervals
    max_time = max(times)
    ax1.set_yticks(np.arange(0, max_time + 2, 2))

    # Customize tick parameters and labels
    ax1.tick_params(axis='y', labelcolor=color)
    ax1.legend(loc='upper right')

    # Set title and layout
    plt.title('Inference Time for Different KNN Models')
    fig.tight_layout()

    # Save the figure to a file
    plt.savefig("assignments/1/figures/plot_modeltime.png")

# FOR ACCURACY skearn by default uses most commom, i changes it to weighted sum ,which gives better accuracy
#TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK 
# WITH :k: 15, Distance Metric: Manhattan, Prediction Type: Weighted Sum
# UNCOMMENT BELOW TO TEST
# task_2_5_2()
# OUTPUT
# Sklearn KNN accuracy 0.115, time 2.5698461532592773
# New KNN accuracy 0.115, time 17.124598026275635
# Old KNN accuracy 0.115, time 22.935333490371704



###############################


















def task_2_5_3():
    # Load the split datasets
    train_df = pd.read_csv('./data/interim/spotify_train.csv')
    test_df = pd.read_csv('./data/interim/spotify_test.csv')
    val_df = pd.read_csv('./data/interim/spotify_val.csv')

    # Extract target variables
    Y_train = train_df['track_genre'].values
    Y_val = val_df['track_genre'].values

    # Initialize hyperparameters
    pred_type = ['weighted_sum']#weighted_sum
    
    # Feature combinations
    feature_combinations = [
        ('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo')
    ]

    def evaluate_knn(pred, feature_set, optimized=0):
        X_train_subset = train_df[list(feature_set)].values
        X_val_subset = val_df[list(feature_set)].values

        if optimized == 1:
            knn = KNearestNeighbours(k=15, distance_metric='manhattan', prediction_type=pred)
        elif optimized == 0:
            knn = Old_KNearestNeighbours(k=15, distance_metric='manhattan', prediction_type=pred)
        elif optimized == 2:
            knn = KNeighborsClassifier(n_neighbors=15, metric='manhattan',weights='distance') #do weighted sum, remove it ->by default most common
        
        start_time = time.time()
        
        if optimized in [0, 1]:
            knn.fit(X_train_subset, Y_train)
            # metrics = knn.validate(X_val_subset, Y_val)
            # accuracy = metrics['accuracy']
            predictions = knn.predict(X_val_subset) 
            accuracy = np.mean(predictions == Y_val)
        else:
            knn.fit(X_train_subset, Y_train)
            # accuracy = knn.score(X_val_subset, Y_val)
            predictions = knn.predict(X_val_subset)
            accuracy = np.mean(predictions == Y_val)
        
        elapsed_time = time.time() - start_time
        return accuracy, elapsed_time

    def timed_evaluation(optimized):
        return evaluate_knn(pred_type[0], feature_combinations[0], optimized)
    
    results = {}
    # Evaluate different KNN implementations in parallel
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = {
            executor.submit(timed_evaluation, optimized): optimized
            for optimized in [0, 1, 2]
        }
        
        for future in concurrent.futures.as_completed(futures):
            optimized = futures[future]
            try:
                accuracy, elapsed_time = future.result()
                if optimized == 0:
                    label = "Old KNN"
                elif optimized == 1:
                    label = "New KNN"
                elif optimized == 2:
                    label = "Sklearn KNN"
                
                results[label] = elapsed_time
                print(f"{label} accuracy {accuracy}, time {elapsed_time}")
            except Exception as e:
                print(f"Error occurred: {e}")

    
    print("train_sample =",0.8*(args.data_sample), results)

    

# PYTHON ASSIGNMENTS.1.A1 --DATA_SAMPLE ,IT PRINTS VALUE, this WAY I GATHERED FOR DIFF DATA SAMPLES
# task_2_5_3()
# OUTPUT:
# dataset_sizes = np.array([100, 500, 1000, 5000, 10000, 50000, 114000])
# old_knn_times = np.array([0.00275, 0.06090, 0.23844, 6.88793, 24.84275, 591.44909, 3029.85131])
# new_knn_times = np.array([0.00234, 0.06524, 0.21555, 6.87011, 17.39268, 120.09435, 314.80442])
# sklearn_knn_times = np.array([0.00225, 0.00583, 0.20349,  1.230361, 1.99732, 3.39953, 5.46194])
def plot_2_5_3():
    dataset_sizes = np.array([100, 500, 1000, 5000, 10000, 50000, 114000])
    old_knn_times = np.array([0.00275, 0.06090, 0.23844, 6.88793, 24.84275, 591.44909, 3029.85131])
    new_knn_times = np.array([0.00234, 0.06524, 0.21555, 6.87011, 17.39268, 120.09435, 314.80442])
    sklearn_knn_times = np.array([0.00225, 0.00583, 0.20349,  1.230361, 1.99732, 3.39953, 5.46194])

    # Create the log-log plot
    plt.figure(figsize=(10, 6))
    plt.plot(dataset_sizes, old_knn_times, marker='o', label='Old KNN Time')
    plt.plot(dataset_sizes, new_knn_times, marker='o', label='New KNN Time')
    plt.plot(dataset_sizes, sklearn_knn_times, marker='o', label='Sklearn KNN Time')

    # Logarithmic scale
    plt.xscale('log')
    plt.yscale('log')

    # Labels and title
    plt.xlabel('Training Dataset Size (log scale)')
    plt.ylabel('Inference Time (s) (log scale)')
    plt.title('Inference Time vs. Training Dataset Size (Log-Log Plot)')
    plt.legend()
    plt.grid(True, which="both", ls="--")
    plt.savefig("assignments/1/figures/plot_timedata.png")

# TASK 2_5_3 CONTINUED PLOT VALUES OBTAINED
# TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK 
#UNCOMMET BELOW
# plot_2_5_3()





















#  2.6 A Second Dataset
#DONE

def task_2_6():
    # Load the split datasets
    train_df = pd.read_csv('./data/external/spotify-2/train.csv')
    test_df = pd.read_csv('./data/external/spotify-2/test.csv')
    val_df = pd.read_csv('./data/external/spotify-2/validate.csv')

    # Extract target variables
    Y_train = train_df['track_genre'].values
    Y_val = val_df['track_genre'].values

    # Initialize hyperparameters
    pred_type = ['weighted_sum']#weighted_sum
    
    # Feature combinations
    feature_combinations = [
        ('danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness', 'valence', 'tempo')
    ]

    def evaluate_knn(pred, feature_set, optimized=0):
        X_train_subset = train_df[list(feature_set)].values
        X_val_subset = val_df[list(feature_set)].values

        if optimized:
            knn = KNearestNeighbours(k=15, distance_metric='manhattan', prediction_type=pred)
        
        knn.fit(X_train_subset, Y_train)
        metrics = knn.validate(X_val_subset, Y_val)
        
        return metrics

    print(f" measures {evaluate_knn(pred_type[0],feature_combinations[0],optimized=1)}")
               
#TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK TASK 
# WITH :k: 15, Distance Metric: Manhattan, Prediction Type: Weighted Sum
# UNCOMMENT BELOW TO TEST
# task_2_6()
# OUTPUT :
# TAKES 21 SEC
# same accuracy by sklearn
#  measures {'macro_precision': 0.1501911362929458, 'macro_recall': 0.1510026846327197, 'macro_f1': 0.14675549714933445, 'micro_precision': 0.15, 'micro_recall': 0.15, 'micro_f1': 0.15, 'accuracy': 0.9850877192982456}










##################################################################################################33
# LINEAR REGRESSION



import pandas as pd
import numpy as np
from models.linear_regression.linear_regression import *

# Load the linreg.csv file
# df = pd.read_csv(r'./data/external/linreg.csv', skiprows=1, names=['x', 'y'], encoding='utf-8')

# Shuffle the rows of the DataFrame
# df = df.sample(frac=1, random_state=42).reset_index(drop=True)
# print(df.head())

# Convert dataframe to numpy array
data = df.values

# Calculate split indices
total_size = len(data)
train_size = int(0.8 * total_size)
temp_size = total_size - train_size
val_size = test_size = int(0.1 * total_size)

# Split the data
train_data = data[:train_size] #just data , numbers
temp_data = data[train_size:]
test_data = temp_data[:test_size]
val_data = temp_data[test_size:]

# Convert numpy arrays back to pandas dataframes
train_df = pd.DataFrame(train_data, columns=['x', 'y'])
test_df = pd.DataFrame(test_data, columns=['x', 'y'])
val_df = pd.DataFrame(val_data, columns=['x', 'y'])

# Save the splits into separate CSV files
train_df.to_csv('./data/interim/linreg_train.csv', index=False)
test_df.to_csv('./data/interim/linreg_test.csv', index=False)
val_df.to_csv('./data/interim/linreg_val.csv', index=False)


############################### saving splitted data 

# Load the split datasets
train_df = pd.read_csv('./data/interim/linreg_train.csv')
test_df = pd.read_csv('./data/interim/linreg_test.csv')
val_df = pd.read_csv('./data/interim/linreg_val.csv')

# Convert data to numpy arrays
X_train = train_df['x'].values.astype(float)
Y_train = train_df['y'].values.astype(float)
X_test = test_df['x'].values.astype(float)
Y_test = test_df['y'].values.astype(float)
X_val = val_df['x'].values.astype(float)
Y_val = val_df['y'].values.astype(float)

#####

def task_3_1_0():
    print("Training Metrics:")
    print("Variance (Train):", np.var(Y_train))
    print("Standard Deviation (Train):", np.std(Y_train))

    print("\nValidation Metrics:")
    print("Variance (Validation):", np.var(Y_val))
    print("Standard Deviation (Validation):", np.std(Y_val))

    print("\nTest Metrics:")
    print("Variance (Test):", np.var(Y_test))
    print("Standard Deviation (Test):", np.std(Y_test))
#####

# task_3_1_0()

# Example usage:





def subtask1(best_lr):
    print("Training Metrics:")
    print("Variance (Train):", np.var(Y_train))
    print("Standard Deviation (Train):", np.std(Y_train))

    print("\nTest Metrics:")
    print("Variance (Validation):", np.var(Y_test))
    print("Standard Deviation (Validation):", np.std(Y_test))

    model = PolynomialLinearRegression(degree=1, learning_rate=best_lr, max_epochs=10000, tolerance=1e-6)
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    coef= model.get_coefficients()
    coef = model.get_coefficients()
    model.plot(X_train,Y_train)
    np.savetxt('models/linear_regression/model_coefficients.txt', coef, header='Coefficients')
    print("MSE test:", model.mse(Y_test, y_pred))


def task_3_1_1():
    learning_rates = [ 0.15, 0.2, 0.4,0.01, 0.05, 0.1]
    best_lr = None
    best_epoch = float('inf')

    for lr in learning_rates:
        # print(f"Testing learning rate: {lr}")
        model = PolynomialLinearRegression(degree=1, learning_rate=lr, max_epochs=10000, tolerance=1e-6)
        model.fit(X_train, Y_train)
        y_pred = model.predict(X_test)
        converged_epoch = model.epoch_converged()
        
        # print(f"Predictions: {y_pred}")
        print(f"lr {lr} Converged at epoch: {converged_epoch}")

        # Keep track of the best learning rate (lowest epoch where the model converges)
        if converged_epoch is not None and converged_epoch < best_epoch:
            best_epoch = converged_epoch
            best_lr = lr

    print(f"Best learning rate: {best_lr} with convergence at epoch {best_epoch}")
    
    subtask1(best_lr)


# UNCOMMENT 
# task_3_1_1()
# OUTPUT 
# Best learning rate: 0.4 with convergence at epoch 42



def mainncode1():
    model = PolynomialLinearRegression(degree=1, learning_rate=0.1, max_epochs=10000, tolerance=1e-6)
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    print("Converged at epoch:", model.epoch_converged())
    # print("Predictions:", y_pred)
    print("Coefficients:", model.get_coefficients())
    print("MSE test:", model.mse(X_test, y_pred))
    print("MSE Validation:", model.mse(X_val, y_pred))
    print("Variance:", model.variance(y_pred))
    print("Standard Deviation:", model.std_dev(y_pred))
    model.plot(X_train, Y_train)  # Visualize training data and model fit




def task_3_1_2():
    k_lis = [ 1,2,3,4,5,6,7,11,20]
    best_k = None
    least_mse = float('inf')

    for k in k_lis:
        # print(f"Testing learning rate: {lr}")
        model = PolynomialLinearRegression(degree=k, learning_rate=0.1, max_epochs=10000, tolerance=1e-6)
        model.fit(X_train, Y_train)
        y_pred = model.predict(X_test)
        mse_= model.mse(y_pred,Y_test)
        # print(f"Predictions: {y_pred}")
        print(f"k {k} has mse : {mse_}")
        print("Variance:", model.variance(y_pred))
        print("Standard Deviation:", model.std_dev(y_pred))
        # model.plot(X_train,Y_train)
        print("Converged at epoch:", model.epoch_converged())

        # Keep track of the best learning rate (lowest epoch where the model converges)
        if  mse_< least_mse:
            least_mse = mse_
            best_k = k

    print(f"Best k that minimizes error {best_k} with mse {least_mse}")
    

# task_3_1_2()
# use in report with mse,variance,sandard_dev
# get other values from this 
# k 1 has mse : 0.3955414037679096
# Converged at epoch: 175
# k 2 has mse : 0.20317968760591124
# Converged at epoch: 628
# k 3 has mse : 0.060824746836441825
# Converged at epoch: 2865
# k 4 has mse : 0.06311303694044934
# Converged at epoch: 9038
# k 5 has mse : 0.02882053124336109
# Converged at epoch: 10000
# k 6 has mse : 0.029477110976911237
# Converged at epoch: 10000
# k 7 has mse : 0.02329466802441688
# Converged at epoch: 10000
# k 11 has mse : 0.019224303049431346
# Converged at epoch: 10000
# Best k that minimizes error 11 with mse 0.019224303049431346
# Reason , overfitting




def task_3_1_3():
    klis= [1,2,3,4,5,6,7,9]
    for k in klis:        
        model = PolynomialLinearRegression(degree=k, learning_rate=0.2, max_epochs=1000, tolerance=1e-6)
        model.fit(X_train, Y_train)
        # model.plot(X_train, Y_train)
        print(model.epoch_converged())
        model.create_animation(X_train,Y_train,save_path= f'assignments/1/figures/gif/convergence_k{k}.gif')

#animation
#UNCOMMENT
# task_3_1_3()
# Line convering is accurate as per epoch, metrices are 1-150 epoch only-> better visualtization (as scale changes, point only visible)


#///////////////////////////////////////-/-/-/-/-/-/-/REGULARISATION REGULARISATION REGULARISATION REGULARISATION REGULARISATION 
# 3.2 Regularization





# Load the regularisation.csv file
df = pd.read_csv(r'./data/external/regularisation.csv', skiprows=1, names=['x', 'y'], encoding='utf-8')

# Shuffle the rows of the DataFrame
# df = df.sample(frac=1, random_state=42).reset_index(drop=True)
print(df.head())

# Convert dataframe to numpy array
data = df.values

# Calculate split indices
total_size = len(data)
train_size = int(0.8 * total_size)
temp_size = total_size - train_size
val_size = test_size = int(0.1 * total_size)

# Split the data
train_data = data[:train_size] #just data , numbers
temp_data = data[train_size:]
test_data = temp_data[:test_size]
val_data = temp_data[test_size:]

# Convert numpy arrays back to pandas dataframes
train_df = pd.DataFrame(train_data, columns=['x', 'y'])
test_df = pd.DataFrame(test_data, columns=['x', 'y'])
val_df = pd.DataFrame(val_data, columns=['x', 'y'])

# Save the splits into separate CSV files
train_df.to_csv('./data/interim/regularisation_train.csv', index=False)
test_df.to_csv('./data/interim/regularisation_test.csv', index=False)
val_df.to_csv('./data/interim/regularisation_val.csv', index=False)

################################ data splitted 
# Load the split datasets
train_df = pd.read_csv('./data/interim/regularisation_train.csv')
test_df = pd.read_csv('./data/interim/regularisation_test.csv')
val_df = pd.read_csv('./data/interim/regularisation_val.csv')

# Convert data to numpy arrays
X_train = train_df['x'].values.astype(float)
Y_train = train_df['y'].values.astype(float)
X_test = test_df['x'].values.astype(float)
Y_test = test_df['y'].values.astype(float)
X_val = val_df['x'].values.astype(float)
Y_val = val_df['y'].values.astype(float)





import matplotlib.pyplot as plt
import numpy as np

def plot_comparison(X_train, Y_train, y_preds, titles, save_path):
    plt.figure(figsize=(10, 6))
    
    # Plotting the data points
    plt.scatter(X_train, Y_train, color='blue', label='Data Points')
    
    # Create a consistent range of x values for plotting the fitted curves
    x_range = np.linspace(X_train.min(), X_train.max(), 100)
    
    for y_pred_curve, title in zip(y_preds, titles):
        # Use the y_pred_curve provided to plot the line
        plt.plot(x_range, y_pred_curve, label=title)
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('Polynomial Regression with Different Regularizations(Lambda=0.01)')
    plt.legend()
    plt.savefig(save_path)  # Save the figure
    plt.close()





def task_3_2(lamb=0.01):
    # Initialize lists for predictions and labels
    y_preds = []
    titles = []
    lamb=0.01  #lambda for all
    # Create a consistent range of x values for plotting
    x_range = np.linspace(X_train.min(), X_train.max(), 100)

    print("No Regularisation")
    model = PolynomialLinearRegression(degree=20, learning_rate=0.1, max_epochs=10000, tolerance=1e-6,regularization_type=None,lambda_=lamb)#None,L1,L2
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    print("Converged at epoch:", model.epoch_converged())
    # print("Predictions:", y_pred)
    # print("Coefficients:", model.get_coefficients())
    print("MSE test:", model.mse(X_test, y_pred))
    print("Variance:", model.variance(y_pred))
    print("Standard Deviation:", model.std_dev(y_pred))
    # model.plot(X_train, Y_train)  # Visualize training data and model fit
    y_pred_curve = model.predict(x_range)  # Predict over x_range for plotting
    y_preds.append(y_pred_curve)
    titles.append("No Regularisation")


    print("L1 Regularisation")
    model = PolynomialLinearRegression(degree=20, learning_rate=0.1, max_epochs=10000, tolerance=1e-6,regularization_type='L1',lambda_=lamb)#None,L1,L2
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    print("Converged at epoch:", model.epoch_converged())
    # print("Predictions:", y_pred)
    # print("Coefficients:", model.get_coefficients())
    print("MSE test:", model.mse(X_test, y_pred))
    print("Variance:", model.variance(y_pred))
    print("Standard Deviation:", model.std_dev(y_pred))
    # model.plot(X_train, Y_train)  # Visualize training data and model fit
    y_pred_curve = model.predict(x_range)  # Predict over x_range for plotting
    y_preds.append(y_pred_curve)
    titles.append("L1 Regularisation")


    print("L2 Regularisation")
    model = PolynomialLinearRegression(degree=20, learning_rate=0.1, max_epochs=10000, tolerance=1e-6,regularization_type='L2',lambda_=lamb)#None,L1,L2
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)
    print("Converged at epoch:", model.epoch_converged())
    # print("Predictions:", y_pred)
    # print("Coefficients:", model.get_coefficients())
    print("MSE test:", model.mse(X_test, y_pred))
    print("Variance:", model.variance(y_pred))
    print("Standard Deviation:", model.std_dev(y_pred))
    # model.plot(X_train, Y_train)  # Visualize training data and model fit
    y_pred_curve = model.predict(x_range)  # Predict over x_range for plotting
    y_preds.append(y_pred_curve)
    titles.append("L2 Regularisation")


    plot_comparison(X_train, Y_train, y_preds, titles, "assignments/1/figures/regular/comparison_plot.png")


# L1 and L2 regularization and compaRISON the results AT DIFF LAMBDA 0.01,0.1,0.5,1 and report
#UNCOMMENT 
# task_3_2(lamb=0.01)
 




def task_3_2animation():
    types= ['L1','L2','None']
    for type in types:
        model = PolynomialLinearRegression(degree=10, learning_rate=0.01, max_epochs=1000, tolerance=1e-6,regularization_type=type,lambda_=0.4)
        model.fit(X_train, Y_train)
        # model.plot(X_train, Y_train)
        print(model.epoch_converged())
        model.create_animation(X_train,Y_train,save_path= f'assignments/1/figures/regular/k_10reg_{type}.gif')



#UNCOMMENT ,
# task_3_2animation()






